[general]

BlockArchitecture = DeepAutoEncoderBlockArchitecture
ClassCount = 10

Verbose = True
Multithreaded = False
Distributed = False
DatasetModule = Demos.Datasets.ToyADMOS
# If you are running a non-distributed demo but would like the dataset stored local to the demo code
LocalDataset = True
GenBlockArchitecture = Demos.gen_auc_ba
# Set the number of threads to be used, 0 does not set a limit, -1 sets the number to the number presented
# by the CPU to the OS
ThreadCount = 1
# Please note that if using the GPU you should run the program using a single thread, multi-threaded training usually
# leads to GPU memory issues as GPU memory management struggles to allocate memory between threads.
GPU = True
Log = True

[evolution]

VerboseMutation = False
MutationProbability = 0.5
CrossoverProbability = 0.1
#number of times an architecture is attempted to be mutated should it mutate into an invalid architecture
MutationAttempts = 10
RetrainEveryGeneration = False
# Alpha value used for mutation function reinforcement learning
Alpha = 0.2

# Mutation can happen in one of two ways, either there is an EQUAL probability for every block in the hierarchy or
# via the given (or default) self-mutation PROBABILITY value, eg. 0.5 self-mutation means that each block has a 50% chance to
# mutate itself. As such the value can be used to shift the focus on mutating to either the top or bottom of the
# hierarchy, eg. 0.5 means the second block layer has a 50% chance while the third has a 25% chance, similarly a 0.9
# value gives the second block layer a 9% chance while the third layer has a 81% chance, assuming a hierarchy of depth
# 3.
; MutationMethod = EQUAL
MutationMethod = PROBABILITY
SelfMutationProbability = 0.75

# Certain search processes use a mutation probability that causes mutation to happen at increasingly deeper levels during
# the search progress, eg. simulated annealing. Thus, one can set the change that should be applied to the mutation
# probability with each generation, either positive (shifting mutation lower) or negative (shifting mutation higher).
VariableMutationGenerationalChange = 0.03

PopulationSize = 40
GenerationCount = 32

[output]

FigureTitle = Goal Attainment
SaveIndividuals = True
OutputPrefix = AnomalyDetection

# How often should the generations be visualized
GenerationGap = 1

# Saving individuals can be done, EVERY generation or at a specified INTERVAL, the final generation is always saved
GenerationSave = EVERY
GenerationSaveInterval = 1

[goals]

# number of goals or the length of goal vector
GoalsNumber = 5
# names of the goals tha appear in log file (every 2 goals needs to be separated by a comma and space ", ")
GoalsNames = Parameters, Accuracy, Cycles, Total ROM, Total RAM

### Goal vector
GoalVector = (269992, 82, 960067, 598922, 5417)
### Normalization vector
NormalizationVector = (3000, 1, 10000, 10000,100)

[mlonmcu]

UseMLonMCU=False
metrics = Cycles, Total ROM, Total RAM
platform = mlif
backend = tvmaotplus
target = etiss_pulpino
frontend = tflite
# Choose the postprocesses to apply (choices: filter_cols, rename_cols, features2cols, config2cols, passcfg2cols, 
# visualize, bytes2kb, artifact2cols, analyse_instructions, compare_rows)
postprocess = None
feature = None
configs = None
parllel = 1
progress = False
verbose = False

[filter]

FilterFunction = MinMaxArray
FilterFunctionModule = TensorNAS.FilterFunctions.MinMax

# The multi-objective optimization requires that every objective has a weight that signifies if it should be
# minimized or maximized, as such the weights can be set to be all 'maximized', all 'minimize' or manually set,
# eg. -1, 1, -1
Weights = (1, -1, 1, 1, 1)

[tensorflow]

# -1 meaning to take the absolute length of the input training & test data
# It should be noted that setting the TrainingSample and TestSample sizes is only applicable if the training and test
# data is provided in arrays
TrainingSampleSize = -1
TestSampleSize = -1
# ValidationSplit is used to create a validation generator if training and test data is provided in array for
ValidationSplit = 0.1
Optimizer = adam
Loss = mean_squared_error
Metrics = accuracy
EarlyStopper = False
# If early stopping is enabled one must specify how much patience one wants
Patience=4
StopperMonitor=accuracy
StopperMinDelta=0.005
StopperMode=max

# For 6GB GPU
BatchSize = 512
# For 24GB GPU
# BatchSize = 4096
TestBatchSize = 32
Epochs = 200
QuantizationAware = False